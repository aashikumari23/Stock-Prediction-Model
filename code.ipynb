{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ohk5FfwwcoHQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UW9wt0WbcroT"
      },
      "outputs": [],
      "source": [
        "drive_folder=\"/content/drive/MyDrive/take3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQeUHW5Hcxlu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUM2BiTNc1Sf"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/take3/ind_nifty500list_filtered_final.csv')\n",
        "os.makedirs(f\"{drive_folder}/stock_data\", exist_ok=True)\n",
        "os.makedirs(f\"{drive_folder}/stock_data/train\", exist_ok=True)\n",
        "os.makedirs(f\"{drive_folder}/stock_data/test\", exist_ok=True)\n",
        "os.makedirs(f\"{drive_folder}/stock_data/val\", exist_ok=True)\n",
        "\n",
        "valid_tickers_file = f\"{drive_folder}/valid_tickers.csv\"\n",
        "pkl_file_path = f\"{drive_folder}/stock_data/nifty500_stock_data.pkl\"\n",
        "\n",
        "nifty500_stock_data = {}\n",
        "valid_tickers = []\n",
        "invalid_tickers = []\n",
        "\n",
        "df_tickers = df['Symbol'] + '.NS'\n",
        "\n",
        "if os.path.exists(pkl_file_path):\n",
        "    print(\"Loading existing stock data from Google Drive...\")\n",
        "    with open(pkl_file_path, 'rb') as file:\n",
        "        nifty500_stock_data = pickle.load(file)\n",
        "    print(\"Stock data loaded successfully.\")\n",
        "else:\n",
        "    print(\"No data found so Fetching new stock data...\")\n",
        "    ticker_data_lengths = {}\n",
        "\n",
        "    for ticker in tqdm(df_tickers[:500], desc=\"Downloading \"):\n",
        "        try:\n",
        "            stock = yf.Ticker(ticker)\n",
        "            company_history = stock.history(start=\"2022-01-10\", end=\"2025-01-11\")\n",
        "\n",
        "            if not company_history.empty:\n",
        "                valid_tickers.append(ticker.replace('.NS', ''))\n",
        "                if hasattr(company_history.index, \"tz_localize\"):\n",
        "                    company_history.index = company_history.index.tz_localize(None)\n",
        "\n",
        "                # Reset index and remove unwanted columns\n",
        "                company_history = company_history.reset_index().drop(columns=['Dividends', 'Stock Splits'], errors='ignore')\n",
        "                company_history = company_history.drop(columns=['Open', 'High', 'Low', 'Volume'], errors='ignore')\n",
        "                ticker_data_lengths[ticker] = len(company_history)\n",
        "                print(f\"üìä {ticker} has {len(company_history)} days of data.\")\n",
        "\n",
        "\n",
        "                # Create labels\n",
        "                company_history['Label'] = (company_history['Close'].shift(-1) > company_history['Close']).astype(int)\n",
        "                company_history = company_history.dropna(subset=['Label'])\n",
        "\n",
        "                # Feature Engineering\n",
        "                company_history['Return'] = company_history['Close'].pct_change()\n",
        "                company_history = company_history.fillna(0)\n",
        "\n",
        "                # Normalize features\n",
        "                price_features = ['Close', 'Return']\n",
        "\n",
        "                scaler_price =  StandardScaler()\n",
        "                company_history['nor_Close'] = scaler_price.fit_transform(company_history[['Close']])\n",
        "\n",
        "                # Split into Train, Validation, Test (70%, 15%, 15%)\n",
        "                total_required = 400 + 160 + 182  # 720 days\n",
        "                if len(company_history) >= total_required:\n",
        "                    company_history = company_history[-total_required:]  # take most recent 720 days\n",
        "\n",
        "                    train_data = company_history[:400]\n",
        "                    val_data = company_history[400:560]\n",
        "                    test_data = company_history[560:]\n",
        "                else:\n",
        "                    print(f\"‚ùå Not enough data for {ticker}. Only {len(company_history)} days available.\")\n",
        "                    invalid_tickers.append(ticker)\n",
        "                    continue\n",
        "\n",
        "                # Save in dictionary\n",
        "                nifty500_stock_data[ticker] = {\n",
        "                    'train': train_data,\n",
        "                    'validation': val_data,\n",
        "                    'test': test_data\n",
        "                }\n",
        "\n",
        "                # Save individual stock CSVs\n",
        "                stock_csv_path_train = f\"{drive_folder}/stock_data/train/{ticker.replace('.NS', '')}_train.csv\"\n",
        "                stock_csv_path_val = f\"{drive_folder}/stock_data/val/{ticker.replace('.NS', '')}_val.csv\"\n",
        "                stock_csv_path_test = f\"{drive_folder}/stock_data/test/{ticker.replace('.NS', '')}_test.csv\"\n",
        "                train_data.to_csv(stock_csv_path_train, index=False)\n",
        "                val_data.to_csv(stock_csv_path_val, index=False)\n",
        "                test_data.to_csv(stock_csv_path_test, index=False)\n",
        "            else:\n",
        "                invalid_tickers.append(ticker)\n",
        "                print(f\"No data available for {ticker}.\")\n",
        "        except Exception as e:\n",
        "            invalid_tickers.append(ticker)\n",
        "            print(f\"Error fetching data for {ticker}: {e}\")\n",
        "# Save valid tickers list\n",
        "valid_tickers_df = pd.DataFrame(valid_tickers, columns=['Symbol'])\n",
        "valid_tickers_df.to_csv(valid_tickers_file, index=False)\n",
        "# Save stock data dictionary\n",
        "with open(pkl_file_path, 'wb') as file:\n",
        "    pickle.dump(nifty500_stock_data, file)\n",
        "print(\"\\n‚úÖ Stock data processing complete and saved to Google Drive.\")\n",
        "# Summary\n",
        "print(f\"‚úÖ Valid tickers: {len(valid_tickers)}\")\n",
        "print(f\"‚ùå Invalid tickers: {len(invalid_tickers)}\")\n",
        "print(f\"‚ùå Invalid tickers list: {invalid_tickers}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6U0GXJLc7Xv"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define sequence length\n",
        "SEQ_LENGTH = 30\n",
        "\n",
        "# Paths for datasets\n",
        "drive_folder = \"/content/drive/MyDrive/take3\"\n",
        "train_folder = f\"{drive_folder}/stock_data/train\"\n",
        "val_folder = f\"{drive_folder}/stock_data/val\"\n",
        "test_folder = f\"{drive_folder}/stock_data/test\"\n",
        "\n",
        "# Paths to save embeddings\n",
        "train_embeddings_path = f\"{drive_folder}/LSTM_embeddings/lstm_train_embeddings.pkl\"\n",
        "val_embeddings_path = f\"{drive_folder}/LSTM_embeddings/lstm_val_embeddings.pkl\"\n",
        "test_embeddings_path = f\"{drive_folder}/LSTM_embeddings/lstm_test_embeddings.pkl\"\n",
        "\n",
        "# Load Stock Data\n",
        "def load_stock_data(folder_path):\n",
        "    stock_data = {}\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        if file_name.endswith('.csv'):\n",
        "            stock_symbol = file_name.replace('.csv', '')\n",
        "            stock_df = pd.read_csv(os.path.join(folder_path, file_name))\n",
        "            stock_data[stock_symbol] = stock_df\n",
        "    return stock_data\n",
        "\n",
        "train_data = load_stock_data(train_folder)\n",
        "val_data = load_stock_data(val_folder)\n",
        "test_data = load_stock_data(test_folder)\n",
        "\n",
        "class StockDataset(Dataset):\n",
        "    def __init__(self, stock_data):\n",
        "        self.data = []\n",
        "        for ticker, data in stock_data.items():\n",
        "            prices = data['nor_Close'].values\n",
        "            for i in range(len(prices) - SEQ_LENGTH):\n",
        "                x_seq = prices[i:i + SEQ_LENGTH]\n",
        "                y_seq = prices[i + SEQ_LENGTH]\n",
        "                self.data.append((x_seq, y_seq))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x_seq, y_seq = self.data[idx]\n",
        "        return torch.tensor(x_seq, dtype=torch.float32), torch.tensor(y_seq, dtype=torch.float32)\n",
        "\n",
        "# Create dataset and dataloaders\n",
        "train_dataset = StockDataset(train_data)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_dataset = StockDataset(val_data)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define LSTM Model\n",
        "class StockLSTM(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=64, num_layers=2):\n",
        "        super(StockLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # lstm_out shape: [batch, seq_len, hidden_size]\n",
        "        # h_n shape: [num_layers, batch, hidden_size]\n",
        "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
        "\n",
        "        # For price prediction (still needed for training)\n",
        "        stock_price = self.fc(h_n[-1])\n",
        "\n",
        "        return stock_price, h_n\n",
        "\n",
        "    def get_embedding(self, x):\n",
        "        # This function is specifically for getting embeddings\n",
        "        with torch.no_grad():\n",
        "            _, (h_n, _) = self.lstm(x)\n",
        "            # Return the hidden state from the last layer\n",
        "            return h_n[-1]  # Shape: [batch, hidden_size]\n",
        "\n",
        "model = StockLSTM().to(device)\n",
        "\n",
        "# Training Setup\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "from tqdm import tqdm  # Import tqdm\n",
        "EPOCHS = 10\n",
        "best_val_loss = float('inf')  # Track best validation loss\n",
        "patience = 3  # Number of epochs to wait before stopping\n",
        "no_improve_epochs = 0  # Counter for early stopping\n",
        "\n",
        "# Use tqdm for tracking epochs\n",
        "epoch_progress = tqdm(range(EPOCHS), desc=\"Training Progress\", unit=\"epoch\")\n",
        "\n",
        "for epoch in epoch_progress:\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for x_seq, y_seq in train_dataloader:\n",
        "        x_seq, y_seq = x_seq.unsqueeze(-1).to(device), y_seq.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output, _= model(x_seq)\n",
        "        loss = criterion(output, y_seq.unsqueeze(1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "    # ---------------- Validate the Model ----------------\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for x_seq, y_seq in val_dataloader:\n",
        "            x_seq, y_seq = x_seq.unsqueeze(-1).to(device), y_seq.to(device)\n",
        "            output,_ = model(x_seq)\n",
        "            loss = criterion(output, y_seq.unsqueeze(1))\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "\n",
        "    # Update tqdm description for epoch progress\n",
        "    epoch_progress.set_postfix(train_loss=avg_train_loss, val_loss=avg_val_loss)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Check for best model and early stopping\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        no_improve_epochs = 0  # Reset counter\n",
        "        torch.save(model.state_dict(), f\"{drive_folder}/best_lstm_model.pth\")\n",
        "        print(\"‚úÖ Best model saved!\")\n",
        "    else:\n",
        "        no_improve_epochs += 1\n",
        "\n",
        "    if no_improve_epochs >= patience:\n",
        "        print(\"‚èπÔ∏è Early stopping triggered!\")\n",
        "        break  # Stop training if validation loss doesn't improve\n",
        "\n",
        "print(\"üéØ Training completed! Loading the best model...\")\n",
        "\n",
        "# Load Best Model Before Generating Embeddings\n",
        "best_model = StockLSTM().to(device)\n",
        "\n",
        "best_model.load_state_dict(torch.load(f\"{drive_folder}/best_lstm_model.pth\"))\n",
        "best_model.eval()\n",
        "def generate_embeddings(stock_data, model, save_path, shift_last_day=False):\n",
        "    lstm_embeddings = {}\n",
        "\n",
        "    with tqdm(stock_data.items(), total=len(stock_data), desc=\"Generating Embeddings\") as t:\n",
        "        for ticker, data in t:\n",
        "            # Get normalized closing prices\n",
        "            prices = data['nor_Close'].values\n",
        "\n",
        "            if shift_last_day:\n",
        "                # Use the second-last day as the target\n",
        "                if len(prices) < (SEQ_LENGTH + 1):\n",
        "                    continue  # skip stocks with too few days\n",
        "                seq = prices[-(SEQ_LENGTH + 1):-1]\n",
        "            else:\n",
        "                if len(prices) < SEQ_LENGTH:\n",
        "                    continue\n",
        "                seq = prices[-SEQ_LENGTH:]\n",
        "\n",
        "            # Convert to tensor\n",
        "            stock_tensor = torch.tensor(seq, dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)\n",
        "\n",
        "            # Get embedding (hidden state)\n",
        "            lstm_embedding = model.get_embedding(stock_tensor).squeeze().cpu().numpy()\n",
        "\n",
        "            # Save embedding\n",
        "            lstm_embeddings[ticker] = lstm_embedding\n",
        "\n",
        "    # Save embeddings to file\n",
        "    with open(save_path, 'wb') as file:\n",
        "        pickle.dump(lstm_embeddings, file)\n",
        "    print(f\"‚úÖ LSTM embeddings saved to {save_path}\")\n",
        "print(\"\\nüîπ Generating LSTM embeddings for Train set...\")\n",
        "generate_embeddings(train_data, best_model, train_embeddings_path)\n",
        "print(\"\\nüîπ Generating LSTM embeddings for Validation set...\")\n",
        "generate_embeddings(val_data, best_model, val_embeddings_path)\n",
        "print(\"\\nüîπ Generating LSTM embeddings for Test set (shifted)...\")\n",
        "generate_embeddings(test_data, best_model, test_embeddings_path, shift_last_day=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoQhq86HdDiu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ========== SET SEED ==========\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(1)\n",
        "\n",
        "# ========== CONFIG ==========\n",
        "drive_path = \"/content/drive/MyDrive/take3\"\n",
        "embedding_dir = os.path.join(drive_path, \"LSTM_embeddings\")\n",
        "label_dir = os.path.join(drive_path, \"stock_data\")\n",
        "model_path = os.path.join(drive_path, \"mlp_regressor_best_model.pth\")\n",
        "\n",
        "embedding_paths = {\n",
        "    \"train\": os.path.join(embedding_dir, \"lstm_train_embeddings.pkl\"),\n",
        "    \"val\": os.path.join(embedding_dir, \"lstm_val_embeddings.pkl\"),\n",
        "    \"test\": os.path.join(embedding_dir, \"lstm_test_embeddings.pkl\"),\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ========== DATASET ==========\n",
        "class EmbeddingRegressionDataset(Dataset):\n",
        "    def __init__(self, embeddings_dict, split):\n",
        "        self.samples = []\n",
        "        for stock, emb in embeddings_dict.items():\n",
        "            stock_name = stock.replace(f\"_{split}\", \"\")\n",
        "            csv_path = os.path.join(label_dir, split, f\"{stock_name}_{split}.csv\")\n",
        "            try:\n",
        "                df = pd.read_csv(csv_path)\n",
        "                ret = df[\"Return\"].values[-2]\n",
        "                label = df[\"Label\"].values[-2]\n",
        "                self.samples.append((torch.tensor(emb, dtype=torch.float32), ret, label, stock))\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping {stock} due to missing data or error: {e}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y, label, stock = self.samples[idx]\n",
        "        return x, torch.tensor(y, dtype=torch.float32), label, stock\n",
        "\n",
        "# ========== MODEL ==========\n",
        "class MLPRegressor(nn.Module):\n",
        "    def __init__(self, input_dim=64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze()\n",
        "\n",
        "# ========== LOAD EMBEDDINGS ==========\n",
        "def load_embeddings(path):\n",
        "    with open(path, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "# ========== TRAIN MODEL ==========\n",
        "def train_regressor(train_loader, val_loader):\n",
        "    model = MLPRegressor().to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    best_model = None\n",
        "    best_val_loss = float(\"inf\")\n",
        "\n",
        "    for epoch in range(50):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for x, y, _, _ in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            pred = model(x)\n",
        "            loss = criterion(pred, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for x, y, _, _ in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                pred = model(x)\n",
        "                loss = criterion(pred, y)\n",
        "                val_loss += loss.item()\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.6f}, Val Loss = {avg_val_loss:.6f}\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_model = model.state_dict()\n",
        "\n",
        "    # Save best model\n",
        "    torch.save(best_model, model_path)\n",
        "    print(f\"‚úÖ Best model saved at: {model_path}\")\n",
        "    model.load_state_dict(best_model)\n",
        "    return model\n",
        "\n",
        "# ========== EVALUATE TOP-K ==========\n",
        "def evaluate_topk(model, test_loader, k_values=[5, 10, 15, 20]):\n",
        "    model.eval()\n",
        "    all_preds, all_returns, all_labels, all_stocks = [], [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y, label, stock in test_loader:\n",
        "            x = x.to(device)\n",
        "            pred = model(x).cpu().numpy()\n",
        "            y = y.numpy()\n",
        "            all_preds.extend(pred)\n",
        "            all_returns.extend(y)\n",
        "            all_labels.extend(label)\n",
        "            all_stocks.extend(stock)\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_returns = np.array(all_returns)\n",
        "    true_labels = np.array(all_labels)\n",
        "\n",
        "    pred_rank = np.argsort(-all_preds)\n",
        "    return_rank = np.argsort(-all_returns)\n",
        "\n",
        "    stock_to_pred_rank = {all_stocks[i]: rank + 1 for rank, i in enumerate(pred_rank)}\n",
        "    stock_to_return_rank = {all_stocks[i]: rank + 1 for rank, i in enumerate(return_rank)}\n",
        "\n",
        "    print(\"\\nüìä Top-K Evaluation:\\n\")\n",
        "    results = {}\n",
        "\n",
        "    for k in k_values:\n",
        "        topk_pred_idx = pred_rank[:k]\n",
        "        topk_true_idx = return_rank[:k]\n",
        "\n",
        "        topk_pred_stocks = [all_stocks[i] for i in topk_pred_idx]\n",
        "        topk_true_stocks = [all_stocks[i] for i in topk_true_idx]\n",
        "\n",
        "        acc = np.mean(true_labels[topk_pred_idx] == 1)\n",
        "        irr = np.sum(all_returns[topk_pred_idx]) - np.sum(all_preds[topk_pred_idx])\n",
        "        mae = np.mean(np.abs(all_returns[topk_pred_idx] - all_preds[topk_pred_idx]))\n",
        "        mrr = np.sum([1 / stock_to_pred_rank[stock] for stock in topk_true_stocks]) / k\n",
        "        precision = len(set(topk_pred_stocks) & set(topk_true_stocks)) / k\n",
        "\n",
        "        # Store metrics\n",
        "        results[k] = {\n",
        "            \"accuracy\": acc,\n",
        "            \"precision\": precision,\n",
        "            \"mae\": mae,\n",
        "            \"irr\": irr,\n",
        "            \"mrr\": mrr\n",
        "        }\n",
        "\n",
        "        # Print top-k stocks table first\n",
        "        print(f\"üîù Top-{k} Stocks:\")\n",
        "        print(\"Rank\\tStock\\t\\tLabel\\tReturn\\tPredicted\\tTrue Return Rank\")\n",
        "        for rank, idx in enumerate(topk_pred_idx):\n",
        "            print(f\"{rank+1}\\t{all_stocks[idx]}\\t{true_labels[idx]}\\t{all_returns[idx]:.4f}\\t{all_preds[idx]:.4f}\\t\\t\\t{stock_to_return_rank[all_stocks[idx]]}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "    # Print metrics summary\n",
        "    print(\"\\nüìà Summary Metrics:\")\n",
        "    for k in k_values:\n",
        "        m = results[k]\n",
        "        print(f\"Top-{k}: Accuracy={m['accuracy']:.4f}, Precision={m['precision']:.4f}, \"\n",
        "              f\"MRR={m['mrr']:.4f}, IRR={m['irr']:.4f}, MAE={m['mae']:.4f}\")\n",
        "\n",
        "# ========== MAIN ==========\n",
        "if __name__ == \"__main__\":\n",
        "    train_emb = load_embeddings(embedding_paths[\"train\"])\n",
        "    val_emb = load_embeddings(embedding_paths[\"val\"])\n",
        "    test_emb = load_embeddings(embedding_paths[\"test\"])\n",
        "\n",
        "    train_set = EmbeddingRegressionDataset(train_emb, \"train\")\n",
        "    val_set = EmbeddingRegressionDataset(val_emb, \"val\")\n",
        "    test_set = EmbeddingRegressionDataset(test_emb, \"test\")\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=64, shuffle=False)\n",
        "    val_loader = DataLoader(val_set, batch_size=64, shuffle=False)\n",
        "    test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
        "\n",
        "    if os.path.exists(model_path):\n",
        "        print(\"üì¶ Loading saved MLP Regressor...\")\n",
        "        model = MLPRegressor().to(device)\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "        model.eval()\n",
        "    else:\n",
        "        print(\"üöÄ Training MLP Regressor...\")\n",
        "        model = train_regressor(train_loader, val_loader)\n",
        "\n",
        "    print(\"üß™ Evaluating on Test Set...\")\n",
        "    evaluate_topk(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fy99NmJdK8Q"
      },
      "outputs": [],
      "source": [
        "def export_all_predictions(model, test_loader, save_path):\n",
        "    model.eval()\n",
        "    all_preds, all_returns, all_labels, all_stocks = [], [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y, label, stock in test_loader:\n",
        "            x = x.to(device)\n",
        "            pred = model(x).cpu().numpy()\n",
        "            y = y.numpy()\n",
        "            all_preds.extend(pred)\n",
        "            all_returns.extend(y)\n",
        "            all_labels.extend(label)\n",
        "            all_stocks.extend(stock)\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        \"Stock\": all_stocks,\n",
        "        \"TrueReturn\": all_returns,\n",
        "        \"PredictedReturn\": all_preds,\n",
        "        \"Label\": all_labels\n",
        "    })\n",
        "\n",
        "    # Sort by predicted return (descending)\n",
        "    df_sorted = df.sort_values(by=\"PredictedReturn\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "    # Save to CSV\n",
        "    df_sorted.to_csv(save_path, index=False)\n",
        "    print(f\"‚úÖ Full prediction ranking saved to: {save_path}\")\n",
        "\n",
        "# Run it\n",
        "csv_output_path = \"/content/drive/MyDrive/take3/mlp_all_predictions.csv\"\n",
        "export_all_predictions(model, test_loader, csv_output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZCT8LKzdQfM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import euclidean\n",
        "from fastdtw import fastdtw\n",
        "\n",
        "# Define parameters\n",
        "SEQ_LENGTH = 6\n",
        "WINDOW_SIZE = 30\n",
        "STEP_SIZE = 6\n",
        "N_CLUSTERS = 5\n",
        "\n",
        "# Paths\n",
        "drive_folder = \"/content/drive/MyDrive/take3\"\n",
        "data_folders = {\n",
        "    \"train\": f\"{drive_folder}/stock_data/train\",\n",
        "    \"test\": f\"{drive_folder}/stock_data/test\",\n",
        "    \"val\": f\"{drive_folder}/stock_data/val\"\n",
        "}\n",
        "motifs_folders = {\n",
        "    \"train\": f\"{drive_folder}/motifs/train\",\n",
        "    \"test\": f\"{drive_folder}/motifs/test\",\n",
        "    \"val\": f\"{drive_folder}/motifs/val\"\n",
        "}\n",
        "modis_folders = {\n",
        "    \"train\": f\"{drive_folder}/Modis/train\",\n",
        "    \"test\": f\"{drive_folder}/Modis/test\",\n",
        "    \"val\": f\"{drive_folder}/Modis/val\"\n",
        "}\n",
        "\n",
        "# Ensure folders exist\n",
        "for folder in list(motifs_folders.values()) + list(modis_folders.values()):\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "\n",
        "# Load CSV stock data into dictionary\n",
        "def load_stock_data(folder_path):\n",
        "    stock_data = {}\n",
        "    files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
        "    for file_name in tqdm(files, desc=\"üì• Loading stock data\"):\n",
        "        try:\n",
        "            stock_symbol = file_name.replace('.csv', '')\n",
        "            stock_df = pd.read_csv(os.path.join(folder_path, file_name))\n",
        "            stock_data[stock_symbol] = stock_df\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to load {file_name}: {e}\")\n",
        "    return stock_data\n",
        "\n",
        "\n",
        "# Compute 1NN using DTW (consistent with medoid selection)\n",
        "def compute_1nn_distances(motifs_A, motifs_B):\n",
        "    return np.array([min(fastdtw(motif_A, motif_B)[0] for motif_B in motifs_B) for motif_A in motifs_A])\n",
        "\n",
        "\n",
        "# Compute frequency-weighted ModIS similarity\n",
        "def calculate_modis_distance(motifs_i, freqs_i, motifs_j, freqs_j):\n",
        "    P_i_to_j = compute_1nn_distances(motifs_i, motifs_j)\n",
        "    P_j_to_i = compute_1nn_distances(motifs_j, motifs_i)\n",
        "\n",
        "    P_i_j = np.concatenate((P_i_to_j, P_j_to_i))\n",
        "    K_i_j = np.concatenate((freqs_i, freqs_j))\n",
        "\n",
        "    return np.exp(-np.sum(P_i_j * K_i_j) / np.sum(K_i_j))\n",
        "\n",
        "\n",
        "# Extract motifs from sliding windows (only if not already present)\n",
        "def extract_motifs_dynamic(stock_data, window_size, seq_length, step_size, n_clusters, save_folder, split):\n",
        "    for ticker, data in tqdm(stock_data.items(), desc=f\"üîç Checking motifs for {split}\"):\n",
        "        prices = data['nor_Close'].values\n",
        "\n",
        "        for start in range(0, len(prices) - window_size + 1, step_size):\n",
        "            motif_file = os.path.join(save_folder, f\"{ticker}_start{start}_motifs.pkl\")\n",
        "            if os.path.exists(motif_file):\n",
        "                continue  # Skip if already exists\n",
        "\n",
        "            window_data = prices[start:start + window_size]\n",
        "            subsequences = [window_data[i:i + seq_length] for i in range(len(window_data) - seq_length + 1)]\n",
        "            subsequences = np.array(subsequences)\n",
        "\n",
        "            if len(subsequences) < 2:\n",
        "                continue\n",
        "\n",
        "            local_clusters = min(n_clusters, len(subsequences))\n",
        "            kmeans = KMeans(n_clusters=local_clusters, random_state=42, n_init=10)\n",
        "            labels = kmeans.fit_predict(subsequences)\n",
        "\n",
        "            unique_labels, counts = np.unique(labels, return_counts=True)\n",
        "            stock_motifs = []\n",
        "            motif_frequencies_ordered = []\n",
        "\n",
        "            for label in unique_labels:\n",
        "                cluster_seqs = subsequences[labels == label]\n",
        "                medoid = min(cluster_seqs, key=lambda x: np.sum([fastdtw(x, y)[0] for y in cluster_seqs]))\n",
        "                stock_motifs.append(medoid)\n",
        "                motif_frequencies_ordered.append(counts[label])\n",
        "\n",
        "            motifs_dict = {'motifs': np.array(stock_motifs), 'frequencies': np.array(motif_frequencies_ordered)}\n",
        "\n",
        "            with open(motif_file, 'wb') as f:\n",
        "                pickle.dump(motifs_dict, f)\n",
        "\n",
        "\n",
        "# Compute ModIS per window (only missing files)\n",
        "def compute_modis_for_windows(motifs_folder, modis_folder, split):\n",
        "    motif_files = [f for f in os.listdir(motifs_folder) if f.endswith('_motifs.pkl')]\n",
        "    start_windows = range(0, 400 - WINDOW_SIZE + 1, STEP_SIZE)\n",
        "\n",
        "    print(f\"üìå Computing ModIS distances for {split}...\")\n",
        "\n",
        "    for start in tqdm(start_windows, desc=f\"üßÆ ModIS for {split}\"):\n",
        "        modis_file = os.path.join(modis_folder, f\"modis_start{start}_{split}.pkl\")\n",
        "        if os.path.exists(modis_file):\n",
        "            print(\"Skiiped\")\n",
        "            continue  # Skip if file exists\n",
        "\n",
        "        stock_motifs = {}\n",
        "        for file in motif_files:\n",
        "            if f\"_start{start}_\" in file:\n",
        "                stock_name = file.replace(f\"_start{start}_motifs.pkl\", '')\n",
        "                with open(os.path.join(motifs_folder, file), 'rb') as f:\n",
        "                    stock_motifs[stock_name] = pickle.load(f)\n",
        "\n",
        "        if len(stock_motifs) < 2:\n",
        "            continue\n",
        "\n",
        "        stock_symbols = list(stock_motifs.keys())\n",
        "        modis_distances = {}\n",
        "\n",
        "        for i in range(len(stock_symbols)):\n",
        "            for j in range(i + 1, len(stock_symbols)):\n",
        "                s_i, s_j = stock_symbols[i], stock_symbols[j]\n",
        "                m_i, f_i = stock_motifs[s_i]['motifs'], stock_motifs[s_i]['frequencies']\n",
        "                m_j, f_j = stock_motifs[s_j]['motifs'], stock_motifs[s_j]['frequencies']\n",
        "\n",
        "                distance = calculate_modis_distance(m_i, f_i, m_j, f_j)\n",
        "                modis_distances[(s_i, s_j)] = distance\n",
        "                modis_distances[(s_j, s_i)] = distance  # Symmetric\n",
        "\n",
        "        with open(modis_file, 'wb') as f:\n",
        "            pickle.dump(modis_distances, f)\n",
        "\n",
        "        print(f\"‚úÖ Saved ModIS for {split} start {start}: {len(stock_motifs)} stocks\")\n",
        "# üíª MAIN EXECUTION\n",
        "for split in ['train','test', 'val']:\n",
        "    print(f\"\\nüöÄ Processing {split.upper()} dataset...\")\n",
        "\n",
        "    stock_data = load_stock_data(data_folders[split])\n",
        "    print(f\"‚úÖ Loaded {len(stock_data)} stocks for {split}.\")\n",
        "\n",
        "    extract_motifs_dynamic(\n",
        "        stock_data,\n",
        "        window_size=WINDOW_SIZE,\n",
        "        seq_length=SEQ_LENGTH,\n",
        "        step_size=STEP_SIZE,\n",
        "        n_clusters=N_CLUSTERS,\n",
        "        save_folder=motifs_folders[split],\n",
        "        split=split\n",
        "    )\n",
        "    print(f\"‚úÖ Motif extraction check/complete for {split}.\")\n",
        "\n",
        "    compute_modis_for_windows(motifs_folders[split], modis_folders[split], split)\n",
        "    print(f\"‚úÖ ModIS computation check/complete for {split}.\")\n",
        "\n",
        "print(\"\\nüéâ‚úÖ All processing completed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItohSOvPdX9b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "def check_and_clean_modis_folder(modis_folder, split_name):\n",
        "    total = 0\n",
        "    empty = 0\n",
        "    empty_files = []\n",
        "\n",
        "    for file in sorted(os.listdir(modis_folder)):\n",
        "        if file.endswith(f\"_{split_name}.pkl\"):\n",
        "            total += 1\n",
        "            file_path = os.path.join(modis_folder, file)\n",
        "            with open(file_path, 'rb') as f:\n",
        "                data = pickle.load(f)\n",
        "                if not data:\n",
        "                    empty += 1\n",
        "                    empty_files.append(file)\n",
        "                    os.remove(file_path)  # Delete empty file\n",
        "                    print(f\"üóëÔ∏è Deleted empty file: {file}\")\n",
        "\n",
        "    print(f\"\\nüîç {split_name.upper()} ModIS Summary\")\n",
        "    print(f\"   ‚Ä¢ Total checked: {total}\")\n",
        "    print(f\"   ‚Ä¢ Empty + deleted: {empty}\")\n",
        "    if empty:\n",
        "        print(\"   ‚Ä¢ Names of deleted files:\")\n",
        "        for f in empty_files:\n",
        "            print(f\"     - {f}\")\n",
        "\n",
        "# Example usage\n",
        "check_and_clean_modis_folder(\"/content/drive/MyDrive/take3/Modis/train\", \"train\")\n",
        "check_and_clean_modis_folder(\"/content/drive/MyDrive/take3/Modis/test\", \"test\")\n",
        "check_and_clean_modis_folder(\"/content/drive/MyDrive/take3/Modis/val\", \"val\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jG-Z3e8lddiI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "agg_path = \"/content/drive/MyDrive/take3/aggregated sheet.xlsx\"\n",
        "\n",
        "# Manually assign column names\n",
        "agg_df = pd.read_excel(agg_path, header=None)\n",
        "agg_df.columns = [\"Fund Name\", \"Company Name\", \"Sector\"]\n",
        "\n",
        "print(\"‚úÖ Columns renamed:\")\n",
        "print(agg_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVLWJ-77dhWt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import time\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Mount Google Drive ---\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- File paths ---\n",
        "TICKER_FILE = \"/content/drive/MyDrive/take3/ind_nifty500list_filtered_final.csv\"\n",
        "AGG_FILE = \"/content/drive/MyDrive/take3/aggregated sheet.xlsx\"\n",
        "OUTPUT_FILE = \"/content/drive/MyDrive/take3/prior_relationship_matrix.npy\"\n",
        "\n",
        "# --- Load ticker and aggregated data ---\n",
        "tickers_df = pd.read_csv(TICKER_FILE)\n",
        "tickers = tickers_df[\"Symbol\"].dropna().unique().tolist()\n",
        "\n",
        "agg_df = pd.read_excel(AGG_FILE, header=None)\n",
        "agg_df.columns = [\"Fund Name\", \"Company Name\", \"Sector\"]\n",
        "\n",
        "# --- Normalize company names for matching ---\n",
        "agg_df['Company Name'] = (\n",
        "    agg_df['Company Name']\n",
        "    .str.lower()\n",
        "    .str.replace(r'\\blimited\\b|\\bltd\\b', 'ltd', regex=True)\n",
        "    .str.replace(r'[^\\w\\s]', '', regex=True)\n",
        "    .str.strip()\n",
        ")\n",
        "\n",
        "# --- Fetch sector/industry from yfinance with delay and retries ---\n",
        "stock_info = {}\n",
        "for ticker in tqdm(tickers, desc=\"Fetching yfinance info\"):\n",
        "    for attempt in range(3):  # retry up to 3 times\n",
        "        try:\n",
        "            yf_ticker = ticker + \".NS\"\n",
        "            data = yf.Ticker(yf_ticker).info\n",
        "            name = data.get(\"longName\", \"\").lower().replace(\"limited\", \"ltd\").replace(\".\", \"\").replace(\",\", \"\").strip()\n",
        "            sector = data.get(\"sector\", \"\")\n",
        "            industry = data.get(\"industry\", \"\")\n",
        "            stock_info[ticker] = {\"name\": name, \"sector\": sector, \"industry\": industry}\n",
        "            time.sleep(0.5)  # delay to avoid rate limits\n",
        "            break\n",
        "        except Exception as e:\n",
        "            if attempt == 2:\n",
        "                print(f\"‚ö†Ô∏è Failed for {ticker}: {e}\")\n",
        "            time.sleep(2)  # wait longer before retry\n",
        "\n",
        "# --- Build prior relationship matrix ---\n",
        "N = len(tickers)\n",
        "G = np.zeros((N, N))\n",
        "\n",
        "for i in tqdm(range(N), desc=\"Building matrix\"):\n",
        "    for j in range(i + 1, N):\n",
        "        ti, tj = tickers[i], tickers[j]\n",
        "        info_i = stock_info.get(ti)\n",
        "        info_j = stock_info.get(tj)\n",
        "\n",
        "        if not info_i or not info_j:\n",
        "            continue\n",
        "\n",
        "        name_i = info_i[\"name\"]\n",
        "        name_j = info_j[\"name\"]\n",
        "\n",
        "        # Mutual fund co-holdings (m)\n",
        "        funds_i = set(agg_df[agg_df['Company Name'] == name_i]['Fund Name'])\n",
        "        funds_j = set(agg_df[agg_df['Company Name'] == name_j]['Fund Name'])\n",
        "        m = len(funds_i & funds_j)\n",
        "\n",
        "        # Sector/industry overlap (n)\n",
        "        n = int(info_i[\"sector\"] == info_j[\"sector\"] or info_i[\"industry\"] == info_j[\"industry\"])\n",
        "\n",
        "        # Total prior relationship g = m + n\n",
        "        G[i, j] = G[j, i] = m + n\n",
        "\n",
        "# --- Save matrix to Drive ---\n",
        "np.save(OUTPUT_FILE, G)\n",
        "print(f\"‚úÖ Prior relationship matrix saved to: {OUTPUT_FILE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0qMN2Gkdm3Q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Paths\n",
        "TICKER_FILE = \"/content/drive/MyDrive/take3/ind_nifty500list_filtered_final.csv\"\n",
        "MATRIX_FILE = \"/content/drive/MyDrive/take3/prior_relationship_matrix.npy\"\n",
        "CSV_OUTPUT_FILE = \"/content/drive/MyDrive/take3/prior_relationship_matrix.csv\"\n",
        "\n",
        "# Load tickers and matrix\n",
        "tickers = pd.read_csv(TICKER_FILE)[\"Symbol\"].dropna().unique().tolist()\n",
        "matrix = np.load(MATRIX_FILE)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(matrix, index=tickers, columns=tickers)\n",
        "\n",
        "# Save as CSV\n",
        "df.to_csv(CSV_OUTPUT_FILE)\n",
        "\n",
        "print(f\"‚úÖ Matrix saved as CSV to: {CSV_OUTPUT_FILE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-fAedEsdqBL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ----- CONFIG -----\n",
        "MODIS_DIR = \"/content/drive/MyDrive/take3/Modis\"\n",
        "SAVE_DIR = \"/content/drive/MyDrive/take3/DGLSTM_outputs\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "SPLITS = ['train', 'val', 'test']\n",
        "STEPS_PER_SPLIT = {'train': 62, 'val': 22, 'test': 26}\n",
        "STEP_SIZE = 6\n",
        "STARTS = {\n",
        "    split: list(range(0, STEPS_PER_SPLIT[split] + 1, STEP_SIZE))\n",
        "    for split in SPLITS\n",
        "}\n",
        "HIDDEN_DIM = 256\n",
        "EPOCHS = 30\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ----- LOAD STOCK ORDER -----\n",
        "valid_tickers = list(pd.read_csv(\"/content/drive/MyDrive/take3/valid_tickers.csv\")['Symbol'])\n",
        "if \"WIPRO\" not in valid_tickers:\n",
        "    valid_tickers.append(\"WIPRO\")\n",
        "valid_tickers = sorted(valid_tickers)\n",
        "stock_index = {ticker: i for i, ticker in enumerate(valid_tickers)}\n",
        "num_stocks = len(valid_tickers)\n",
        "\n",
        "# ----- LOAD PRIOR -----\n",
        "prior_matrix = np.load(\"/content/drive/MyDrive/take3/prior_relationship_matrix.npy\")\n",
        "prior_tensor = torch.tensor(prior_matrix, dtype=torch.float32, device=DEVICE)\n",
        "\n",
        "# ----- Load ModIS Sequences -----\n",
        "def load_modis_sequence(split):\n",
        "    modis_sequence = []\n",
        "    print(f\"üìÇ Loading ModIS sequence for '{split}'...\")\n",
        "    for start in tqdm(STARTS[split], desc=f\"‚è≥ {split.upper()}\"):\n",
        "        path = os.path.join(MODIS_DIR, split, f\"modis_start{start}_{split}.pkl\")\n",
        "        with open(path, 'rb') as f:\n",
        "            edges = pickle.load(f)\n",
        "\n",
        "        adj = np.zeros((num_stocks, num_stocks), dtype=np.float32)\n",
        "        for (s1, s2), val in edges.items():\n",
        "            s1_clean = s1.split(\"_\")[0]\n",
        "            s2_clean = s2.split(\"_\")[0]\n",
        "            if s1_clean in stock_index and s2_clean in stock_index:\n",
        "                i, j = stock_index[s1_clean], stock_index[s2_clean]\n",
        "                adj[i, j] = val\n",
        "\n",
        "        modis_sequence.append(torch.tensor(adj, dtype=torch.float32))\n",
        "    return modis_sequence\n",
        "\n",
        "# ----- Model -----\n",
        "class DGLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(DGLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, batch_first=True)\n",
        "        self.proj = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "    def forward(self, graphs, prior):\n",
        "        T, N, _ = graphs.shape\n",
        "        fused_graphs = []\n",
        "        for t in range(T):\n",
        "            decay = torch.exp(-torch.tensor(t / T, dtype=torch.float32, device=DEVICE))\n",
        "            fused = graphs[t] * (1 - decay) + decay * prior\n",
        "            fused_graphs.append(fused.unsqueeze(0))\n",
        "        fused_graphs = torch.cat(fused_graphs, dim=0)\n",
        "        lstm_input = fused_graphs.permute(1, 0, 2)  # [N, T, N]\n",
        "        output, _ = self.lstm(lstm_input)\n",
        "        final_hidden = output[:, -1, :]\n",
        "        dynamic_graph = self.proj(final_hidden)\n",
        "        return F.relu(dynamic_graph)\n",
        "\n",
        "# ----- Training -----\n",
        "def train_dglstm(train_seq, val_seq, prior_tensor, num_epochs=30, lr=1e-3):\n",
        "    model = DGLSTM(input_dim=num_stocks, hidden_dim=HIDDEN_DIM).to(DEVICE)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    train_tensor = torch.stack(train_seq).to(DEVICE)  # [T_train, N, N]\n",
        "    val_tensor = torch.stack(val_seq).to(DEVICE)      # [T_val, N, N]\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_path = os.path.join(SAVE_DIR, \"best_dglstm_model.pt\")\n",
        "\n",
        "    print(\"üß† Training DGLSTM model...\")\n",
        "    for epoch in tqdm(range(num_epochs), desc=\"üìö Epochs\"):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        pred_train = model(train_tensor, prior_tensor)\n",
        "        train_loss = F.mse_loss(pred_train, train_tensor[-1])\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            pred_val = model(val_tensor, prior_tensor)\n",
        "            val_loss = F.mse_loss(pred_val, val_tensor[-1])\n",
        "\n",
        "        if val_loss.item() < best_val_loss:\n",
        "            best_val_loss = val_loss.item()\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "\n",
        "        if epoch % 5 == 0:\n",
        "            print(f\"üìù Epoch {epoch}: Train Loss = {train_loss.item():.4f} | Val Loss = {val_loss.item():.4f}\")\n",
        "            print(f\"Train Target stats: min={train_tensor[-1].min():.4f}, max={train_tensor[-1].max():.4f}, mean={train_tensor[-1].mean():.4f}\")\n",
        "            print(f\"Predicted Graph stats: min={pred_train.min():.4f}, max={pred_train.max():.4f}, mean={pred_train.mean():.4f}\")\n",
        "\n",
        "    return best_model_path\n",
        "\n",
        "# ----- Pipeline -----\n",
        "dglstm_graphs = {}\n",
        "\n",
        "# Load training + validation sequences\n",
        "train_seq = load_modis_sequence('train')\n",
        "val_seq = load_modis_sequence('val')\n",
        "full_train_seq = train_seq + val_seq\n",
        "\n",
        "# Train & load best model\n",
        "best_model_path = train_dglstm(full_train_seq, val_seq, prior_tensor, num_epochs=EPOCHS)\n",
        "best_model = DGLSTM(input_dim=num_stocks, hidden_dim=HIDDEN_DIM).to(DEVICE)\n",
        "best_model.load_state_dict(torch.load(best_model_path))\n",
        "best_model.eval()\n",
        "\n",
        "# Generate graph for each split\n",
        "for split in SPLITS:\n",
        "    print(f\"üìà Generating graph for {split}...\")\n",
        "    modis_seq = load_modis_sequence(split)\n",
        "    graph_tensor = torch.stack(modis_seq).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        graph = best_model(graph_tensor, prior_tensor)\n",
        "    dglstm_graphs[split] = graph.cpu().numpy()\n",
        "    np.save(os.path.join(SAVE_DIR, f\"dglstm_graph_{split}.npy\"), dglstm_graphs[split])\n",
        "\n",
        "print(\"‚úÖ All dynamic graphs saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyUk2cM-dxnN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch_geometric.nn import GATConv\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_score\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Paths\n",
        "base_path = \"/content/drive/MyDrive/take3\"\n",
        "embedding_path = os.path.join(base_path, \"LSTM_embeddings/lstm_{}_embeddings.pkl\")\n",
        "graph_path = os.path.join(base_path, \"DGLSTM_outputs/dglstm_graph_{}.npy\")\n",
        "label_path = \"/content/drive/MyDrive/take3/stock_data/{split}/{company}_{split}.csv\"\n",
        "\n",
        "def load_embeddings(split):\n",
        "    with open(embedding_path.format(split), \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def load_graph(split, stock_list):\n",
        "    adj = np.load(graph_path.format(split))\n",
        "    edge_index, edge_weight = [], []\n",
        "    for i in range(len(stock_list)):\n",
        "        for j in range(len(stock_list)):\n",
        "            if adj[i, j] != 0:\n",
        "                edge_index.append([i, j])\n",
        "                edge_weight.append(adj[i, j])\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "    edge_weight = torch.tensor(edge_weight, dtype=torch.float)\n",
        "    return edge_index, edge_weight\n",
        "\n",
        "def load_labels(split, stock_list):\n",
        "    labels, returns = [], []\n",
        "    for stock in tqdm(stock_list, f\"Preparing for {split}\"):\n",
        "        csv_path = label_path.format(split=split, company=stock.replace(f\"_{split}\", \"\"))\n",
        "        df = pd.read_csv(csv_path)\n",
        "        if \"Label\" not in df.columns or \"Return\" not in df.columns:\n",
        "            raise ValueError(f\"'Label' or 'Return' column not found in {csv_path}\")\n",
        "        labels.append(df[\"Label\"].values[-2])\n",
        "        returns.append(df[\"Return\"].values[-2])\n",
        "    return torch.tensor(labels, dtype=torch.long), np.array(returns)\n",
        "\n",
        "class GATPredictor(nn.Module):\n",
        "    def __init__(self, in_dim=64, hidden_dim=64, heads=4):\n",
        "        super(GATPredictor, self).__init__()\n",
        "        self.gat1 = GATConv(in_dim, hidden_dim, heads=heads, dropout=0.4)\n",
        "        self.gat2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, dropout=0.4)\n",
        "        self.fc = nn.Linear(hidden_dim * heads, 1)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight):\n",
        "        x = self.gat1(x, edge_index, edge_weight)\n",
        "        x = self.gat2(x, edge_index, edge_weight)\n",
        "        enhanced = x.clone()\n",
        "        out = self.fc(enhanced).squeeze()\n",
        "        return out, enhanced\n",
        "\n",
        "def prepare_data(split):\n",
        "    embeddings = load_embeddings(split)\n",
        "    stock_list = sorted(embeddings.keys())\n",
        "    features = np.stack([embeddings[s] for s in stock_list])\n",
        "    norms = np.linalg.norm(features, axis=1, keepdims=True)\n",
        "    features = features / np.clip(norms, 1e-8, None)\n",
        "    x = torch.tensor(features, dtype=torch.float).to(device)\n",
        "    edge_index, edge_weight = load_graph(split, stock_list)\n",
        "    y, returns = load_labels(split, stock_list)\n",
        "    return x, edge_index.to(device), edge_weight.to(device), y.to(device), returns, stock_list\n",
        "\n",
        "def train_model():\n",
        "    train_x, train_ei, train_ew, train_y, _, _ = prepare_data(\"train\")\n",
        "    val_x, val_ei, val_ew, val_y, _, _ = prepare_data(\"val\")\n",
        "    test_x, test_ei, test_ew, test_y, test_returns, test_stock_list = prepare_data(\"test\")\n",
        "\n",
        "    # Pos weight for imbalance handling\n",
        "    pos_weight = torch.tensor([(train_y == 0).sum() / (train_y == 1).sum()], device=device)\n",
        "\n",
        "    model = GATPredictor().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-3, weight_decay=1e-4)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "    best_model = None\n",
        "    best_val_loss = float(\"inf\")\n",
        "    train_losses, val_losses = [], []\n",
        "    patience, counter = 10, 0\n",
        "\n",
        "    print(\"Training GAT model...\")\n",
        "    for epoch in tqdm(range(50), desc=\"Epochs\"):\n",
        "        model.train()\n",
        "        logits, _ = model(train_x, train_ei, train_ew)\n",
        "        loss = criterion(logits, train_y.float())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_logits, _ = model(val_x, val_ei, val_ew)\n",
        "            val_loss = criterion(val_logits, val_y.float())\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "        val_losses.append(val_loss.item())\n",
        "        print(f\"Epoch {epoch+1:02d}: Train Loss = {loss.item():.4f}, Val Loss = {val_loss.item():.4f}\")\n",
        "\n",
        "        if val_loss.item() < best_val_loss:\n",
        "            best_val_loss = val_loss.item()\n",
        "            best_model = model.state_dict()\n",
        "            best_train_loss = loss.item()\n",
        "            best_epoch = epoch + 1\n",
        "            counter = 0\n",
        "            print(f\"‚úÖ New best model at epoch {best_epoch}: Train Loss = {best_train_loss:.4f}, Val Loss = {best_val_loss:.4f}\")\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience and epoch >= 20:\n",
        "                print(\"üî• Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    torch.save(best_model, os.path.join(base_path, \"gat_best_model.pth\"))\n",
        "    print(f\"‚úÖ Best model saved at: {os.path.join(base_path, 'gat_best_model.pth')}\")\n",
        "    print(f\"üèÜ Selected Best Epoch = {best_epoch}, Train Loss = {best_train_loss:.4f}, Val Loss = {best_val_loss:.4f}\")\n",
        "\n",
        "\n",
        "    model.load_state_dict(best_model)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_logits, enhanced = model(test_x, test_ei, test_ew)\n",
        "        pred_scores = torch.sigmoid(test_logits)/10  # Apply sigmoid for evaluation only\n",
        "        np.save(os.path.join(base_path, \"enhanced_test_embeddings.npy\"), enhanced.cpu().numpy())\n",
        "        print(\"üì¶ Enhanced test embeddings saved.\")\n",
        "\n",
        "    return pred_scores.cpu().numpy(), test_y.cpu().numpy(), test_returns, test_stock_list, enhanced.cpu().numpy()\n",
        "\n",
        "# Evaluation unchanged\n",
        "# ... reuse your existing `evaluate(...)` function ...\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "def evaluate(pred_scores, true_labels, return_ratios, stock_list, k_values=[5, 10, 15, 20]):\n",
        "    result = {}\n",
        "    pred_rank = np.argsort(-pred_scores)  # high score ‚Üí higher rank\n",
        "    true_return_rank = np.argsort(-return_ratios)  # high return ‚Üí higher rank\n",
        "\n",
        "    stock_to_pred_rank = {stock_list[idx]: rank + 1 for rank, idx in enumerate(pred_rank)}\n",
        "    stock_to_return_rank = {stock_list[idx]: rank + 1 for rank, idx in enumerate(true_return_rank)}\n",
        "\n",
        "    print(\"\\nTop-K Evaluation (based on predicted scores vs true returns):\")\n",
        "    for k in k_values:\n",
        "        print(f\"\\nTop-{k} stocks:\")\n",
        "        # Top-k by predicted scores\n",
        "        topk_pred_idx = pred_rank[:k]\n",
        "        topk_pred_stocks = [stock_list[i] for i in topk_pred_idx]\n",
        "        topk_pred_labels = true_labels[topk_pred_idx]\n",
        "\n",
        "        # Top-k by actual returns\n",
        "        topk_true_idx = true_return_rank[:k]\n",
        "        topk_true_stocks = [stock_list[i] for i in topk_true_idx]\n",
        "        # Predicted labels (using 0.5 threshold)\n",
        "        topk_predicted_labels = (pred_scores[topk_pred_idx] >= 0.5).astype(int)\n",
        "        topk_true_labels = true_labels[topk_pred_idx]\n",
        "        acc = np.mean(topk_predicted_labels == topk_true_labels)\n",
        "        # Precision = how many predicted top-k are in true top-k return\n",
        "        precision = len(set(topk_pred_stocks) & set(topk_true_stocks)) / k\n",
        "\n",
        "        # MRR = average 1 / predicted rank for the true top-k return stocks\n",
        "        mrr = np.sum([1 / stock_to_pred_rank[stock] for stock in topk_true_stocks])/k\n",
        "\n",
        "        # IRR = average 1 / return rank of model-predicted top-k stocks\n",
        "        irr = np.sum(return_ratios[topk_pred_idx]) - np.sum(pred_scores[topk_pred_idx])\n",
        "\n",
        "        # MAE = mean absolute error between predicted scores and actual returns (for top-k predictions)\n",
        "        mae = np.mean(np.abs(pred_scores[topk_pred_idx] - return_ratios[topk_pred_idx]))\n",
        "\n",
        "        result[k] = {\"accuracy\": acc, \"precision\": precision, \"mrr\": mrr, \"irr\": irr, \"mae\": mae}\n",
        "\n",
        "        print(\"Rank\\tStock\\t\\tLabel\\tScore\\t\\tReturn Rank\\tActual Return\")\n",
        "        for rank, idx in enumerate(topk_pred_idx):\n",
        "            stock = stock_list[idx]\n",
        "            label = true_labels[idx]\n",
        "            score = pred_scores[idx]\n",
        "            return_rank = stock_to_return_rank[stock]\n",
        "            ret_val = return_ratios[idx]\n",
        "            print(f\"{rank+1}\\t{stock:<12}\\t{label}\\t{score:.4f}\\t\\t{return_rank}\\t\\t{ret_val:.4f}\")\n",
        "    print(\"Label 1 count:\", (true_labels == 1).sum())\n",
        "    print(\"Label 0 count:\", (true_labels == 0).sum())\n",
        "    return result\n",
        "if __name__ == \"__main__\":\n",
        "    pred_scores, true_labels, return_ratios, stock_list, final_embeddings = train_model()\n",
        "    results = evaluate(pred_scores, true_labels, return_ratios, stock_list)\n",
        "    print(\"\\nüìä Final Metrics:\")\n",
        "    for k, metrics in results.items():\n",
        "        print(f\"Top-{k}: Accuracy={metrics['accuracy']:.4f}, Precision={metrics['precision']:.4f}, \"\n",
        "              f\"MRR={metrics['mrr']:.4f}, IRR={metrics['irr']:.4f}, MAE={metrics['mae']:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
